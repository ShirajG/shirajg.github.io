---
layout: post
title: Sorting
category: tech
---
        <h4><span>Sorting Algorithms</span></h4>
        <p>
          This weeks post is going to cover the basics of sorting algorithms. Sorting algorithms are a great way to learn about
          some of the more fundamental concepts in programming, and really help you to understand all the different considerations that go into a seemingly simple task. I am going to cover 3 different types of sorting algorithms in this
          post. First up we will cover bubble sort, then merge sort and 
          finally quick sort. Hopefully after reading this you will come away with a better appreciation for seemingly simple programming tasks. 
        </p>
        <p>
          Let's start with a quick overview of what sorting is, exactly. The basic goal of any sorting algorithm is to take a list that is not in size, or alphabetical order, and sort them so they are. This may sound simple at first, but once we get into the details, you will see that its not so simple. 
        </p>
        <p>
          One of the simplest sorting algorithms, and one used in many textbooks as an example, is the "bubble sort". This algorithm works by taking a look at the first two entries in the list and seeing which value is greater. It will then swap the values so they are in ascending order. It then continues to do this process for every pair of values in the list. This has the effect of "bubbling" the largest value up to the end of the list, and "bubbling" smaller values down towards the beginning of the list. This animation should make it more clear:
          <img class="image" src="http://upload.wikimedia.org/wikipedia/commons/c/c8/Bubble-sort-example-300px.gif" alt="Bubble sort">
        </p>
        <p>
          This is a good algorithm for getting people used to the concept of algorithms as a set of repeatable operations, that will always give the desired result, but it is not suited for real world uses. Take the case of a list a billion entries long; you would have to do, on average, 1,000,000,000^2 operations to sort the list. This is because each loop through the list only ever places 1 number in the correct location, so you have no choice but to loop through the list one billion times. Each loop then has to go through every item
          in the list and compare it to its neighbor. This means that for every number that gets moved to its correct place at the end of the list, 1,000,000,000 comparisons have to be done, and since there are 1,000,000,000 numbers in the list, we arrive at 1,000,000,000^2 as the total number of operations. As you can see this is going to be very inefficient for larger lists.
          This brings us to our next topic, merge sort.
        </p>
        <p>
          Merge sort is also a sorting algorithm, but it's approach to the problem of sorting is much different than that of bubble sort. Merge sort uses a "divide and conquer" strategy to accomplish it's sort. What this means is that merge sort divides the problem up into small easily manageable chunks, and then combines the smaller results into a larger complete result. 
          How the algorithm accomplishes it's goal of sorting a list, is to break the list down into the smallest list possible, a list of one item. It then adds this result to the result of sorting its neighbor (also a list of one), and then sorting that list. It then keeps repeating this process of combining itself with its neighboring result, to eventually form the final fully sorted list. Hopefully, this image can help you to visualize how this works:
          <img class='image' src="http://upload.wikimedia.org/wikipedia/commons/c/cc/Merge-sort-example-300px.gif" alt="Merge Sort">
        </p>
        <p>
          This algorithm is much quicker than bubble sort because you no longer need to iterate through the entire list to place just one item in its correct position. The major drawback of merge sort is that it needs a lot more memory than the simpler bubble sort. This is because bubble sort sorts the list in place, meanwhile merge sort has to use more memory in order to store the smaller lists before rejoining them. This is a consideration you have to take into account if you decide to use this algorithm on a large data set, as you need to make sure the system is going to have enough memory to allocate for the needed copies. On a large list with billions of entries, this can be a non trivial amount. The next algorithm, quick sort tries to deal with this issue.
        </p>
        <p>
          Quick sort works in a similar way to merge sort, in that it takes a divide and conquer approach. Quick sort picks a random value in the unordered list, called the "pivot" and then orders the list around this pivot. It goes through the list and places all values smaller than the pivot to the left of the pivot, and all values greater than the pivot to the right. These sub lists on either side of the pivot are called "partitions" and are treated as if they were their own separate lists. The process is repeated for the left partition and then the right partition. You keep repeating this process, of picking a pivot, partitioning the list, and ordering it until the size of the partitions to be sorted reaches 1, then you're done! Check out this animation to help you visualize it, the solid black boxes are the pivots:
          <img class='image' src="http://upload.wikimedia.org/wikipedia/commons/9/9c/Quicksort-example.gif" alt="Quick sort">
        </p>
        <p>
          This works without having to make duplicates of all the values in the unordered list, as you only ever need to swap 2 values at a time. This reduces the memory requirements, and can cause quicksort to be faster than mergesort in many cases. However, one of the drawbacks of quick sort is its reliance upon a randomly chosen pivot. If you happen to be unlucky enough to chose the maximum or minimum value in the list as your first pivot, the algorithm will have to move every value on the list either to the right or to the left of the pivot. This is pretty much the same behavior as we saw in bubble sort. Thankfully in most real world cases, even if you are unlucky enough to pick the minimum or maximum as the first pivot, the next pivot will not be so. However, the extremely slim chance does exist that every pivot you choose ends up being either the min or the max for that particular partition. In this case quick sort would run at exactly the same speed as bubble sort, since only one element is being placed into position at one time. A clever trick to get around this is to base your choice of pivot on the average of the first, middle and last element in the partition. This guarantees that you do not end up picking the minimum or the maximum value from the partition. 
        </p>
        <p>
          As you can see, the logic behind a seemingly simple task like sorting a list can be much more interesting and intricate than you may have suspected. Many other algorithms for sorting exist, and they all have their own strengths and weaknesses. Part of out jobs as developers is to be able to determine which one of these will be most efficient for the job at hand, and try to optimize our implementation of the algorithm.
        </p>
